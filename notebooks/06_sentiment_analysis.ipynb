{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4094646b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import glob\n",
    "import json\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import networkx as nx\n",
    "import os\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf42f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starter analyse af 497 noder...\n",
      "Error reading jasontheween: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "--- Result ---\n",
      "Total VADER coverage: 12.71%\n",
      "\n",
      "Top 20 words VADER don't understands:\n",
      "1: 196712\n",
      "w: 133856\n",
      "lul: 129763\n",
      "om: 120985\n",
      "ww: 112988\n",
      "2: 112374\n",
      "üòÇ: 109552\n",
      "get: 107075\n",
      "u: 102992\n",
      "Õè: 99354\n",
      "emirulove: 95829\n",
      "üò≠: 84132\n",
      "itskay: 83558\n",
      "bro: 76999\n",
      "go: 76480\n",
      "game: 74752\n",
      "wendol: 70126\n",
      "chat: 69829\n",
      "Ô∏è: 68614\n",
      "subscribed: 67967\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Setup\n",
    "G = nx.read_gml('../Proj3/mention_network.gml')\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "tokenizer = TweetTokenizer() \n",
    "vader_lexicon = analyzer.lexicon \n",
    "\n",
    "# filters\n",
    "# 1. stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation) \n",
    "punctuation.update(['...', '..', '‚Äô', '‚Äú', '‚Äù'])\n",
    "\n",
    "# Global counters\n",
    "global_total_tokens = 0\n",
    "global_known_tokens = 0\n",
    "global_unknown_words = Counter()\n",
    "\n",
    "def extract_body(msg):\n",
    "    content = msg.get(\"message\", \"\")\n",
    "    if isinstance(content, dict):\n",
    "        content = content.get(\"body\", \"\")\n",
    "    return content if isinstance(content, str) else \"\"\n",
    "\n",
    "files_dir = \"../Proj3/mention_network_chats/\"\n",
    "\n",
    "for node in G.nodes():\n",
    "    pattern = os.path.join(files_dir, f\"{node.capitalize()}_*.json\")\n",
    "    matches = glob.glob(pattern)\n",
    "    if len(matches) == 0:\n",
    "        pattern = os.path.join(files_dir, f\"{node}_*.json\") \n",
    "        matches = glob.glob(pattern)\n",
    "    \n",
    "    try:\n",
    "        # Check if file exists\n",
    "        if not matches:\n",
    "            continue\n",
    "            \n",
    "        with open(matches[0], 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "    except (IndexError, FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error reading {node}: {e}\")\n",
    "        continue\n",
    "\n",
    "    messages = data.get(\"comments\", [])\n",
    "    \n",
    "    # Processer hver besked\n",
    "    for msg in messages:\n",
    "        text = extract_body(msg)\n",
    "        if not text:\n",
    "            continue\n",
    "            \n",
    "        # Tokenizer og lowercase\n",
    "        tokens = tokenizer.tokenize(text.lower())\n",
    "\n",
    "        # Filter\n",
    "        tokens = [t for t in tokens if t not in stop_words and t not in punctuation]\n",
    "        \n",
    "        for token in tokens:\n",
    "            global_total_tokens += 1\n",
    "            if token in vader_lexicon:\n",
    "                global_known_tokens += 1\n",
    "            else:\n",
    "                # Gem ukendte ord s√• vi kan se, hvad der mangler\n",
    "                global_unknown_words[token] += 1\n",
    "\n",
    "# --- Resultat Beregning ---\n",
    "\n",
    "if global_total_tokens > 0:\n",
    "    coverage_pct = (global_known_tokens / global_total_tokens) * 100\n",
    "    print(f\"\\n--- Result ---\")\n",
    "    print(f\"Total VADER coverage: {coverage_pct:.2f}%\")\n",
    "    \n",
    "    print(f\"\\nTop 20 words VADER don't understands:\")\n",
    "    for word, count in global_unknown_words.most_common(20):\n",
    "        print(f\"{word}: {count}\")\n",
    "else:\n",
    "    print(\"No tokens fund.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddcc2be",
   "metadata": {},
   "source": [
    "### Analysis of Lexicon Coverage on Twitch Data\n",
    "The application of the VADER sentiment analysis tool on the dataset yielded a lexical coverage of only 12.71%. This critically low coverage indicates that nearly 88% of the tokens in the corpus are unrecognized by the model and consequently classified as neutral noise.\n",
    "\n",
    "Analzing the top 20 words, we can see:\n",
    "\n",
    "**Domain-Specific Slang and Emotes:** High-frequency tokens such as lul (laughter), w (win/success), and channel-specific emotes like emirulove and itskay are semantic pillars of Twitch communication but are absent from standard lexicons.\n",
    "\n",
    "**Colloquialisms and Abbreviations:** The prevalence of shorthand such as u (you), bro, and om illustrates a highly informal, conversational register that standard models often fail to parse correctly without normalization.\n",
    "\n",
    "**Platform Noise:** A significant portion of the \"language\" consists of non-conversational artifacts, including system messages (subscribed), spam (1, 2), and raw Unicode characters (Õè), which dilute the sentiment signal.\n",
    "\n",
    "This findings demonstrate that Twitch chat operates with a highly specialized and internal sociolect that is significantly distinct from the standard social media language (e.g., Twitter) that VADER was trained on. Consequently, performing sentiment analysis using off-the-shelf VADER without substantial domain adaptation (lexicon injection) lacks validity, as the majority of sentiment-bearing tokens are being systematically ignored."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialgraphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
