{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab731194",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import os\n",
    "from glob import glob\n",
    "import json\n",
    "import emoji\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78688826",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = os.path.dirname(os.getcwd())\n",
    "data_path = os.path.join(root_path, \"data\\\\mention_network_chats\\\\\")\n",
    "data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd1cd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "\n",
    "twitch_system_words = { # Common Twitch system messages\n",
    "    'welcome', 'joined', 'left', 'hosted', 'hosting',\n",
    "    'subscribed', 'gifted', 'sub', 'prime', 'tier', \n",
    "    'months', 'streak', 'resubscribed', 'raided'\n",
    "}\n",
    "\n",
    "stopwords += list(twitch_system_words)\n",
    "\n",
    "chat_filler_stopwords = {\n",
    "    'lol', 'lmao', 'like', 'hai', 'mhm', 'yeah', 'yawn', \n",
    "    'bro', 'back', 'get', 'one', 'good', 'want', 'think', \n",
    "    'know', 'im', 'thats', 'dont', 'cant', 'tho', 'oh', 'uh',\n",
    "    'ok', 'okay', 'right', 'maybe', 'well', 'see', 'bot',\n",
    "    'watch', 'streams', 'turn', 'stop', 'please', 'got', 'ass', 'fire', 'call',\n",
    "    'kekw', 'lul', 'pog', 'poggers', 'monka', 'ez', 'monkas', 'pepe', 'xd', \n",
    "    'omegalul', 'gasm', 'hi'\n",
    "\n",
    "}\n",
    "\n",
    "stopwords += list(chat_filler_stopwords)\n",
    "\n",
    "# Define your spam phrases globally outside the functions\n",
    "# Add any suspected bot/alert phrases confirmed from your investigation\n",
    "SPAM_PHRASES = {\n",
    "    'subscribed with prime', \n",
    "    'subscribed for',\n",
    "    'just subscribed',\n",
    "    'gifted a subscription', \n",
    "    'gifted a sub',\n",
    "    'gifted sub',\n",
    "    'gift',\n",
    "    'donate',\n",
    "    'consecutive streams'\n",
    "}\n",
    "\n",
    "\n",
    "def generate_alias_set(streamer_name):\n",
    "    \"\"\"\n",
    "    Generates a set of potential aliases by simplifying the complex streamer name.\n",
    "    \"\"\"\n",
    "    aliases = {streamer_name.lower()}\n",
    "    \n",
    "    # 1. Remove leading/trailing numbers and symbols (2xrakai -> rakai)\n",
    "    # This targets names like 'Lirik123' or 'XqcOW'\n",
    "    simple_name = re.sub(r'(\\d+|tv|live|gaming|ow|hd|_|-)$', '', streamer_name, flags=re.IGNORECASE)\n",
    "    simple_name = re.sub(r'^(\\d+|tv|live|gaming|ow|hd|_|-)', '', simple_name, flags=re.IGNORECASE)\n",
    "\n",
    "    if simple_name.lower() != streamer_name.lower() and len(simple_name) >= 3:\n",
    "        aliases.add(simple_name.lower())\n",
    "    \n",
    "    # 2. Generate prefixes of the simplified name (rakai -> rak, raka, rakai)\n",
    "    # This catches the original 'aceu' -> 'ace'\n",
    "    if len(simple_name) >= 3:\n",
    "        for i in range(3, len(simple_name) + 1):\n",
    "            aliases.add(simple_name[:i].lower())\n",
    "            \n",
    "    # 3. Heuristic for complex names: The common short alias is often the last part.\n",
    "    # This specifically targets 'rakai' -> 'kai'\n",
    "    if len(simple_name) > 4:\n",
    "        short_alias = simple_name[-3:].lower() # Take the last 3 characters\n",
    "        if short_alias.isalpha(): # Only include if it's not just numbers/symbols\n",
    "            aliases.add(short_alias)\n",
    "            \n",
    "    return aliases\n",
    "\n",
    "\n",
    "def is_auto_message(text):\n",
    "    \"\"\"\n",
    "    Checks if a message body contains known system/alert phrases.\n",
    "    \"\"\"\n",
    "    lowered_text = text.lower()\n",
    "    \n",
    "    # Check for known system phrases\n",
    "    for phrase in SPAM_PHRASES:\n",
    "        if phrase in lowered_text:\n",
    "            return True\n",
    "            \n",
    "    # Optional: Check for short, common non-human phrases \n",
    "    # (e.g., just 'hi' or 'xD' might pass this, but you can adjust)\n",
    "    # if len(lowered_text.split()) < 2:\n",
    "    #    return True \n",
    "\n",
    "    return False\n",
    "\n",
    "def squash_spam(token):\n",
    "    \n",
    "    if len(token) > 2:\n",
    "        # Check for 3 or more consecutive identical characters\n",
    "        pattern = r'(.)\\1{2,}'\n",
    "        if re.search(pattern, token):\n",
    "            return True\n",
    "\n",
    "\n",
    "def remove_mentions(text):\n",
    "    \"\"\"\n",
    "    Removes all user mentions (e.g., @MukkingAround, @OmegaTooYew) \n",
    "    from the text, replacing them with a space.\n",
    "    \"\"\"\n",
    "    # Regex pattern: Looks for the '@' symbol followed by one or more \n",
    "    # non-whitespace characters, up to the end of the word/line.\n",
    "    # It handles common Twitch names (which often include letters, numbers, and underscores).\n",
    "    pattern = r'@\\S+'\n",
    "    return re.sub(pattern, ' ', text).strip()\n",
    "\n",
    "\n",
    "def remove_streamer_name(text, streamer):\n",
    "    if not streamer:\n",
    "        return text\n",
    "\n",
    "    aliases_to_remove = generate_alias_set(streamer)\n",
    "\n",
    "    # Escape and sort by length descending\n",
    "    escaped_aliases = [re.escape(alias) for alias in aliases_to_remove]\n",
    "    escaped_aliases.sort(key=len, reverse=True)\n",
    "    \n",
    "    pattern_str = '|'.join(escaped_aliases)\n",
    "    \n",
    "    # Simple, high-impact substitution:\n",
    "    cleaned_text = re.sub(pattern_str, \" \", text, flags=re.IGNORECASE) \n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def normalize_urls(text):\n",
    "    url_pattern = r'https?://\\S+|www\\.\\S+'\n",
    "    return re.sub(url_pattern, '', text)\n",
    "\n",
    "def preprocess_text(text, streamer):\n",
    "\n",
    "    text = normalize_urls(text)\n",
    "    text = remove_mentions(text)\n",
    "    text = remove_streamer_name(text, streamer)\n",
    "    text = emoji.demojize(text, delimiters=(\" \", \" \")) # Reformat emojis to text\n",
    "    lowered_text = text.lower()\n",
    "    tokens = nltk.word_tokenize(lowered_text)\n",
    "    tokens = [token for token in tokens if token.isalnum()] # Keep only alphanumeric tokens\n",
    "    tokens = [token for token in tokens if not squash_spam(token)]\n",
    "    tokens = [token for token in tokens if token not in stopwords]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397fcb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all chat files\n",
    "chat_files = list(glob(data_path + '*_chat.json'))\n",
    "tokenized_chat = defaultdict(list)\n",
    "print(f\"Found {len(chat_files)} chat files to analyze\\n\")\n",
    "\n",
    "for i, chat_file in tqdm(enumerate(chat_files), total=len(chat_files)):\n",
    "    # Extract streamer name from filename\n",
    "    filename = chat_file.split(\"\\\\\")[-1].split(\".\")[0]  # Remove .json\n",
    "    source_streamer = filename.split('_')[0].lower()  # First part is streamer name\n",
    "    \n",
    "    try:\n",
    "        with open(chat_file, 'r', encoding='utf-8') as f:\n",
    "            chat_data = json.load(f)       \n",
    "        comments = chat_data.get('comments', [])\n",
    "        for comment in tqdm(comments, desc=f\"Processing comments for {source_streamer}\"):\n",
    "            fragments = comment['message']['fragments']\n",
    "            if not fragments:\n",
    "                continue\n",
    "            if comment['message']['fragments'][0]['emoticon']:\n",
    "                continue\n",
    "            text = comment['message']['fragments'][0]['text']\n",
    "            if is_auto_message(text):\n",
    "                continue  # Skip auto messages\n",
    "            tokens = preprocess_text(text, source_streamer)\n",
    "            tokenized_chat[source_streamer] += tokens\n",
    "        if i == 100:\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(f\"[{i}/{len(chat_files)}] {source_streamer:15s}: Error - {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313117c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_comment = dict(tokenized_chat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140bdbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_map = {i: streamer for i, streamer in enumerate(list(streamer_comment.keys()))} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc332830",
   "metadata": {},
   "outputs": [],
   "source": [
    "streamer_chats = [\" \".join(chat) for chat in list(streamer_comment.values())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3d6455",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()  \n",
    "\n",
    "tfidf = tfidf_vectorizer.fit_transform(streamer_chats)\n",
    "\n",
    "words = tfidf_vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9b587f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# streamer_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a77d1cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(streamer, tfidf_vector):\n",
    "    scores = tfidf_vector.toarray()[0]\n",
    "    word_scores = {words[i]: scores[i] for i in range(len(words))}\n",
    "    \n",
    "    wc = WordCloud(width=800, height=400).generate_from_frequencies(word_scores)\n",
    "    \n",
    "    plt.imshow(wc)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(streamer)\n",
    "    plt.show()\n",
    "for idx in range(len(streamer_comment)):\n",
    "    show_wordcloud(streamer_map[idx], tfidf[idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "socialgraphs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
